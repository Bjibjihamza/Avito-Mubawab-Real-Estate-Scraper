{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44137277-bf03-4837-a85d-2ce902a0411b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (2.31.0)\n",
      "Collecting beautifulsoup4\n",
      "  Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/94/fe/3aed5d0be4d404d12d36ab97e2f1791424d9ca39c2f754a6285d59a3b01d/beautifulsoup4-4.14.2-py3-none-any.whl.metadata\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/14/a0/bb38d3b76b8cae341dad93a2dd83ab7462e6dbcdd84d43f54ee60a8dc167/soupsieve-2.8-py3-none-any.whl.metadata\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from beautifulsoup4) (4.10.0)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/106.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.14.2 soupsieve-2.8\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49f4240-c843-46ae-9a13-d282f800ebc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e35241-ec24-48d7-80e1-6cd3c2bf7720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Garde cette cellule en haut de tous tes notebooks qui √©crivent sur ADLS\n",
    "\n",
    "adls_key = \"....\"\n",
    "\n",
    "spark.conf.set(\n",
    "    \"....\",\n",
    "    adls_key\n",
    ")\n",
    "spark.conf.set(\n",
    "    \"....\",\n",
    "    adls_key\n",
    ")\n",
    "\n",
    "storage_account = \"....\"\n",
    "container = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593fc7f2-87fd-43db-9468-2b0c783c5e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 1 : Imports et Configuration\n",
    "# ========================================\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://www.avito.ma\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0 Safari/537.36\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca58deca-3141-4035-8560-b20415ff4c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 2 : Fonctions basiques\n",
    "# ========================================\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    \"\"\"T√©l√©charge le HTML d'une page Avito.\"\"\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def extract_id_from_url(url: str) -> str:\n",
    "    \"\"\"Extrait l'ID num√©rique √† la fin de l'URL Avito.\"\"\"\n",
    "    m = re.search(r\"_([0-9]+)\\.htm$\", url.split(\"?\")[0])\n",
    "    return m.group(1) if m else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef538575-d61a-4f75-93c0-7ebd5ae87f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 3 : breadcrumbs, cat√©gorie, localisation & date\n",
    "# ========================================\n",
    "\n",
    "def get_breadcrumbs(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"Retourne breadcrumbs_list et breadcrumbs (string)\"\"\"\n",
    "    crumbs = []\n",
    "    ol = soup.find(\"ol\", class_=re.compile(r\"sc-16q833i-0\"))\n",
    "    if ol:\n",
    "        for li in ol.find_all(\"li\", class_=re.compile(r\"sc-16q833i-3\")):\n",
    "            span_or_a = li.find([\"a\", \"span\"])\n",
    "            if span_or_a:\n",
    "                text = span_or_a.get_text(strip=True)\n",
    "                if text:\n",
    "                    crumbs.append(text)\n",
    "\n",
    "    return {\n",
    "        \"breadcrumbs_list\": crumbs,\n",
    "        \"breadcrumbs\": \" > \".join(crumbs) if crumbs else \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def get_category_label(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    R√©cup√®re le libell√© de cat√©gorie du bloc 'Categorie' (ex: 'Appartements, √† louer').\n",
    "    \"\"\"\n",
    "    cat_section = soup.find(\"div\", attrs={\"aria-label\": re.compile(r\"Category \")})\n",
    "    if not cat_section:\n",
    "        return \"\"\n",
    "\n",
    "    texts = cat_section.stripped_strings\n",
    "    for t in texts:\n",
    "        if \"Categorie\" in t:\n",
    "            continue\n",
    "        return t\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_location_and_date(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re location et published_date_raw.\n",
    "    Ajoute √©galement scraping_time comme Mubawab.\n",
    "    \"\"\"\n",
    "    location = \"\"\n",
    "    published_date_raw = \"\"\n",
    "\n",
    "    # Date de publication depuis balise <time>\n",
    "    time_tag = soup.find(\"time\", attrs={\"datetime\": True})\n",
    "    if time_tag:\n",
    "        published_date_raw = time_tag[\"datetime\"]\n",
    "\n",
    "    # Location (ex: Racine, Casablanca)\n",
    "    location_span = None\n",
    "    for svg in soup.find_all(\"svg\", title=re.compile(r\"MapPinFill Icon\")):\n",
    "        parent = svg.parent\n",
    "        location_span = parent.find(\"span\", class_=re.compile(r\"sc-16573058-17\"))\n",
    "        if location_span:\n",
    "            break\n",
    "\n",
    "    if location_span:\n",
    "        location = location_span.get_text(strip=True)\n",
    "\n",
    "    # Temps de scraping (comme Mubawab)\n",
    "    scraping_time = datetime.utcnow()\n",
    "    scraping_time_iso = scraping_time.isoformat()\n",
    "\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"published_date\": published_date_raw,  # Harmonisation avec Mubawab\n",
    "        \"scraping_time\": scraping_time_iso,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdaa6f17-1fa0-419d-aabc-33c8cc0fe667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 4 : titre, prix, description, images\n",
    "# ========================================\n",
    "\n",
    "def get_title_and_price(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"R√©cup√®re le titre (h1) et le prix.\"\"\"\n",
    "    title = \"\"\n",
    "    price_text = \"\"\n",
    "\n",
    "    # Titre\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        title = h1.get_text(strip=True)\n",
    "\n",
    "    # Prix\n",
    "    price_block = soup.find(\"div\", class_=re.compile(r\"sc-16573058-10\"))\n",
    "    if price_block:\n",
    "        p = price_block.find(\"p\")\n",
    "        if p:\n",
    "            price_text = p.get_text(strip=True)\n",
    "    else:\n",
    "        p = soup.find(string=re.compile(r\"DH\"))\n",
    "        if p:\n",
    "            price_text = p.strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price_text\": price_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_description(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"R√©cup√®re la description depuis le bloc Description.\"\"\"\n",
    "    desc_container = None\n",
    "    for div in soup.find_all(\"div\", class_=re.compile(r\"sc-b59a33d2-3\")):\n",
    "        h2 = div.find(\"h2\")\n",
    "        if h2 and \"Description\" in h2.get_text():\n",
    "            desc_container = div\n",
    "            break\n",
    "\n",
    "    if desc_container:\n",
    "        text_parts = []\n",
    "        for node in desc_container.find_all([\"p\", \"div\", \"span\"], recursive=True):\n",
    "            t = node.get_text(\" \", strip=True)\n",
    "            if t:\n",
    "                text_parts.append(t)\n",
    "        description = \" \".join(text_parts)\n",
    "        description = re.sub(r\"\\s+\", \" \", description).strip()\n",
    "        return description\n",
    "\n",
    "    # fallback\n",
    "    p = soup.find(\"p\")\n",
    "    if p:\n",
    "        return p.get_text(\" \", strip=True)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_images(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"R√©cup√®re toutes les URLs d'images de la galerie principale.\"\"\"\n",
    "    urls = []\n",
    "    for img in soup.select(\"div.picture img\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src and \"content.avito.ma/classifieds/images\" in src:\n",
    "            if src not in urls:\n",
    "                urls.append(src)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07196b50-7204-421e-92f6-f55703153389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 5 : attributs, √©quipements, infos vendeur\n",
    "# ========================================\n",
    "\n",
    "def get_attributes_and_equipments(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re :\n",
    "      - attributes: dict { \"Chambres\": \"3\", \"Salle de bain\": \"2\", ... }\n",
    "      - equipments: liste [\"Ascenseur\", \"Balcon\", ...]\n",
    "    \"\"\"\n",
    "    attributes = {}\n",
    "    equipments = []\n",
    "\n",
    "    attr_blocks = soup.find_all(\"div\", class_=re.compile(r\"sc-cd1c365e-0\"))\n",
    "    for block in attr_blocks:\n",
    "        parent = block.find_parent(\"div\", class_=re.compile(r\"sc-b59a33d2-3\"))\n",
    "        is_equip_section = False\n",
    "        if parent:\n",
    "            h2 = parent.find(\"h2\")\n",
    "            if h2 and \"√âquipements\" in h2.get_text():\n",
    "                is_equip_section = True\n",
    "\n",
    "        for item in block.find_all(\"div\", class_=re.compile(r\"sc-cd1c365e-1\")):\n",
    "            img = item.find(\"img\", alt=True)\n",
    "            label_from_alt = img[\"alt\"].strip() if img else \"\"\n",
    "\n",
    "            value_span = item.find(\"span\", class_=re.compile(r\"fjZBup\"))\n",
    "            value = value_span.get_text(strip=True) if value_span else \"\"\n",
    "\n",
    "            if is_equip_section:\n",
    "                if label_from_alt:\n",
    "                    equipments.append(label_from_alt)\n",
    "                elif value:\n",
    "                    equipments.append(value)\n",
    "            else:\n",
    "                if label_from_alt and value:\n",
    "                    attributes[label_from_alt] = value\n",
    "\n",
    "    return {\n",
    "        \"attributes\": attributes,\n",
    "        \"equipments\": equipments,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_seller_info(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"R√©cup√®re seller_name, seller_url, seller_is_store.\"\"\"\n",
    "    seller_name = \"\"\n",
    "    seller_url = \"\"\n",
    "    seller_is_store = False\n",
    "\n",
    "    seller_block = soup.find(\"div\", attrs={\"data-test\": \"av_sellerInfo\"})\n",
    "    if not seller_block:\n",
    "        seller_block = soup.find(\"div\", class_=re.compile(r\"sc-1l0do2b-0\"))\n",
    "\n",
    "    if seller_block:\n",
    "        a = seller_block.find(\"a\", href=True)\n",
    "        if a:\n",
    "            seller_url = urljoin(BASE_URL, a[\"href\"])\n",
    "            name_tag = a.find(\"p\") or a.find(\"span\")\n",
    "            if name_tag:\n",
    "                seller_name = name_tag.get_text(strip=True)\n",
    "\n",
    "        text_block = seller_block.get_text(\" \", strip=True)\n",
    "        if \"Voir la boutique\" in text_block:\n",
    "            seller_is_store = True\n",
    "\n",
    "        if seller_block.find(\"title\", string=re.compile(r\"Store Icon\")):\n",
    "            seller_is_store = True\n",
    "\n",
    "    return {\n",
    "        \"seller_name\": seller_name,\n",
    "        \"seller_url\": seller_url,\n",
    "        \"seller_is_store\": seller_is_store,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd8fa17-84bb-4a0f-ab80-a21dbe65e14d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 6 : Parser principal\n",
    "# ========================================\n",
    "\n",
    "def parse_avito_ad(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"Scrape une annonce Avito et retourne un dict.\"\"\"\n",
    "    html = fetch_html(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    ad_id = extract_id_from_url(url)\n",
    "\n",
    "    crumbs = get_breadcrumbs(soup)\n",
    "    cat_label = get_category_label(soup)\n",
    "    loc_date = get_location_and_date(soup)\n",
    "    title_price = get_title_and_price(soup)\n",
    "    description = get_description(soup)\n",
    "    images = get_images(soup)\n",
    "    attrs_equip = get_attributes_and_equipments(soup)\n",
    "    seller = get_seller_info(soup)\n",
    "\n",
    "    ad_data: Dict[str, Any] = {\n",
    "        \"id\": ad_id,\n",
    "        \"url\": url,\n",
    "        \"title\": title_price[\"title\"],\n",
    "        \"price_text\": title_price[\"price_text\"],\n",
    "        \"location\": loc_date[\"location\"],\n",
    "        \"published_date\": loc_date[\"published_date\"],\n",
    "        \"scraping_time\": loc_date[\"scraping_time\"],\n",
    "        \"breadcrumbs_list\": crumbs[\"breadcrumbs_list\"],\n",
    "        \"breadcrumbs\": crumbs[\"breadcrumbs\"],\n",
    "        \"category_label\": cat_label,\n",
    "        \"description\": description,\n",
    "        \"images\": images,\n",
    "        \"attributes\": attrs_equip[\"attributes\"],\n",
    "        \"equipments\": attrs_equip[\"equipments\"],\n",
    "        \"seller_name\": seller[\"seller_name\"],\n",
    "        \"seller_url\": seller[\"seller_url\"],\n",
    "        \"seller_is_store\": seller[\"seller_is_store\"],\n",
    "    }\n",
    "\n",
    "    return ad_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beeb91b5-bc6a-42f2-bd2c-d41b47349cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 7 : extraire URLs depuis listing\n",
    "# ========================================\n",
    "\n",
    "def get_ad_urls_from_listing(listing_url: str) -> List[str]:\n",
    "    \"\"\"R√©cup√®re toutes les URLs d'annonces d'une page listing Avito.\"\"\"\n",
    "    html = fetch_html(listing_url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    ad_urls = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "\n",
    "        if href.startswith(\"/\"):\n",
    "            href = urljoin(BASE_URL, href)\n",
    "\n",
    "        if \"avito.ma\" not in href:\n",
    "            continue\n",
    "\n",
    "        if re.search(r\"_[0-9]+\\.htm$\", href):\n",
    "            clean = href.split(\"?\")[0]\n",
    "            ad_urls.add(clean)\n",
    "\n",
    "    ad_urls = sorted(ad_urls)\n",
    "    print(f\"üåê Trouv√© {len(ad_urls)} annonces sur {listing_url}\")\n",
    "    return ad_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85eef70f-0184-4472-9a0e-3ecba21dbe40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 8 : Save CSV (optionnel)\n",
    "# ========================================\n",
    "\n",
    "def save_ads_to_csv(ad_dicts: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"Sauvegarde une liste d'annonces (dict) dans un fichier CSV.\"\"\"\n",
    "    if not ad_dicts:\n",
    "        print(f\"Aucune annonce √† sauvegarder pour {filename}\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\n",
    "        \"id\",\n",
    "        \"url\",\n",
    "        \"title\",\n",
    "        \"price_text\",\n",
    "        \"location\",\n",
    "        \"published_date\",\n",
    "        \"scraping_time\",\n",
    "        \"breadcrumbs\",\n",
    "        \"breadcrumbs_list\",\n",
    "        \"category_label\",\n",
    "        \"description\",\n",
    "        \"attributes\",\n",
    "        \"equipments\",\n",
    "        \"seller_name\",\n",
    "        \"seller_url\",\n",
    "        \"seller_is_store\",\n",
    "    ]\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for ad in ad_dicts:\n",
    "            row: Dict[str, Any] = {}\n",
    "\n",
    "            for key in fieldnames:\n",
    "                if key in (\"attributes\", \"equipments\", \"breadcrumbs_list\"):\n",
    "                    continue\n",
    "                row[key] = ad.get(key, \"\")\n",
    "\n",
    "            row[\"attributes\"] = json.dumps(\n",
    "                ad.get(\"attributes\", {}), ensure_ascii=False\n",
    "            )\n",
    "            row[\"equipments\"] = \"; \".join(ad.get(\"equipments\", []))\n",
    "            row[\"breadcrumbs_list\"] = json.dumps(\n",
    "                ad.get(\"breadcrumbs_list\", []), ensure_ascii=False\n",
    "            )\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"‚úÖ Sauvegard√© {len(ad_dicts)} annonces dans {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25211b32-86f5-4a7f-94a7-673fd6200b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 9 : scraper listing -> liste\n",
    "# ========================================\n",
    "\n",
    "def scrape_listing_to_list(listing_url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scrape la 1√®re page d'un listing Avito et retourne une liste de dicts.\"\"\"\n",
    "    ad_urls = get_ad_urls_from_listing(listing_url)\n",
    "\n",
    "    all_ads: List[Dict[str, Any]] = []\n",
    "    total = len(ad_urls)\n",
    "\n",
    "    for i, ad_url in enumerate(ad_urls, 1):\n",
    "        print(f\"[{i}/{total}] Scraping {ad_url}\")\n",
    "        try:\n",
    "            ad_data = parse_avito_ad(ad_url)\n",
    "            all_ads.append(ad_data)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Erreur sur {ad_url}: {e}\")\n",
    "\n",
    "        time.sleep(random.uniform(1.0, 2.5))\n",
    "\n",
    "    return all_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165d6113-fa57-44d0-af8c-ad9048edc4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Trouv√© 38 annonces sur https://www.avito.ma/fr/maroc/ventes_immobilieres-√†_vendre\n",
      "[1/38] Scraping https://www.avito.ma/fr/allal_el_fassi/appartements/Appart_√†_Vendre_126_m¬≤_IZDIHAR_Marrakech_57230172.htm\n",
      "[2/38] Scraping https://www.avito.ma/fr/alliance/appartements/Appartement_Rez_de_chauss√©e_avec_garage_57238259.htm\n",
      "[3/38] Scraping https://www.avito.ma/fr/asilah/appartements/Appartement_√†_vendre_Marina_Assilah_57238366.htm\n",
      "[4/38] Scraping https://www.avito.ma/fr/autre_secteur/appartements/Appartement_Riad_Essalam_Beni_Yekhlef__Mohammedia_37619566.htm\n",
      "[5/38] Scraping https://www.avito.ma/fr/autre_secteur/appartements/Appartement_√†_vendre_76_m¬≤_√†_Bouznika_57112908.htm\n",
      "[6/38] Scraping https://www.avito.ma/fr/autre_secteur/appartements/Appartement_√†_vendre_situ√©_√†_ISKAN_CHAABI_56117952.htm\n",
      "[7/38] Scraping https://www.avito.ma/fr/autre_secteur/terrains_et_fermes/Cinq_hectares_d_oliviers_√†_A√Øn_Johra_Tiflet_54818597.htm\n",
      "[8/38] Scraping https://www.avito.ma/fr/autre_secteur/terrains_et_fermes/Hectare_et_trois_ouvriers_agricoles__bon_march√©__au_milieu_des_fermes_56655901.htm\n",
      "[9/38] Scraping https://www.avito.ma/fr/autre_secteur/terrains_et_fermes/Opportunit√©_en_or___Terrain_√†_vendre_√†_un_excellent_prix_57214923.htm\n",
      "[10/38] Scraping https://www.avito.ma/fr/autre_secteur/terrains_et_fermes/Terrain_titr√©_9000_m√®tres_au_milieu_des_voisins_53884254.htm\n",
      "[11/38] Scraping https://www.avito.ma/fr/autre_secteur/villas_et_riads/Un_lot_de_villa_√†_c√©der_56552847.htm\n",
      "[12/38] Scraping https://www.avito.ma/fr/bab_al_bahr/terrains_et_fermes/Bonne_affaire_57113726.htm\n",
      "[13/38] Scraping https://www.avito.ma/fr/bir_rami/terrains_et_fermes/Magnifique_terrain_R_3_150m2_a_BIR_RAMI_57184451.htm\n",
      "[14/38] Scraping https://www.avito.ma/fr/bir_rami_est/terrains_et_fermes/Terrain_√†_vendre_en_face_de_sews_57134606.htm\n",
      "[15/38] Scraping https://www.avito.ma/fr/centre_ville/villas_et_riads/Villa_√†_vendre_375m2_sur_2_niveaux_centre_ville_55598549.htm\n",
      "[16/38] Scraping https://www.avito.ma/fr/hay_riad/appartements/Appartement_meubl√©_quasi_neuf_57211535.htm\n",
      "[17/38] Scraping https://www.avito.ma/fr/hay_riad/terrains_et_fermes/Terrain_de_440m¬≤_Commerce_49382090.htm\n",
      "[18/38] Scraping https://www.avito.ma/fr/hay_riad/terrains_et_fermes/Terrain_√†_vendre_√†_hayriad_56126279.htm\n",
      "[19/38] Scraping https://www.avito.ma/fr/hay_riad/villas_et_riads/Villa_bonne_affaire_340m¬≤_52123274.htm\n",
      "[20/38] Scraping https://www.avito.ma/fr/hay_riad/villas_et_riads/villa_√†_vendre_derri√®re_nakhil_55963455.htm\n",
      "[21/38] Scraping https://www.avito.ma/fr/hay_salam/appartements/Appartement_Hay_salam_57073771.htm\n",
      "[22/38] Scraping https://www.avito.ma/fr/hivernage/appartements/Appartement_106_m¬≤_aux_portes_de_l_hivernage_56748037.htm\n",
      "[23/38] Scraping https://www.avito.ma/fr/m_hamid/terrains_et_fermes/Sisan_tijariyin_98m_prix_100_milion_2_wajihate_57238264.htm\n",
      "[24/38] Scraping https://www.avito.ma/fr/mabrouka/appartements/Appartement_√†_vendre_111_m¬≤_√†_Marrakech_57146255.htm\n",
      "[25/38] Scraping https://www.avito.ma/fr/malabata/maisons/Maison_moderne_dans_la_r√©gion_de_Bouhachem__route_Moulay_Abd_ÿßŸÑÿ≥ŸÑÿßŸÖ_57146789.htm\n",
      "[26/38] Scraping https://www.avito.ma/fr/ma√¢rif_extension/appartements/Appartement_Duplex_id√©al_pour_Invt__locatif_57202957.htm\n",
      "[27/38] Scraping https://www.avito.ma/fr/mimosas/appartements/Studios_Haut_Standing_√†_Mimosa_57238310.htm\n",
      "[28/38] Scraping https://www.avito.ma/fr/quartier_du_parc/appartements/super_studio_67_m¬≤_√†_Mohammedia_quartier_parc_55597380.htm\n",
      "[29/38] Scraping https://www.avito.ma/fr/route_de_casablanca/appartements/Vente_joli_appartement_vide_57238343.htm\n",
      "[30/38] Scraping https://www.avito.ma/fr/route_de_f√®s/villas_et_riads/Villa_reoyabe_57116525.htm\n",
      "[31/38] Scraping https://www.avito.ma/fr/sidi_bernoussi/appartements/Appartement_Sidi_Bernoussi_R√©sidences_Al_Badr_37621281.htm\n",
      "[32/38] Scraping https://www.avito.ma/fr/sidi_bernoussi/appartements/Appartement_√†_vendre_64_m¬≤_√†_Casablanca_37621456.htm\n",
      "[33/38] Scraping https://www.avito.ma/fr/sidi_bernoussi/appartements/Appartements_√†_vendre_√†_Casablanca_55174614.htm\n",
      "[34/38] Scraping https://www.avito.ma/fr/sidi_bernoussi/appartements/R√©sidence_Prestige_2__Quartier_Al_Azhar_56176008.htm\n",
      "[35/38] Scraping https://www.avito.ma/fr/sidi_yahya_zaer/terrains_et_fermes/Terrain_villa_√†_vendre_57238312.htm\n",
      "[36/38] Scraping https://www.avito.ma/fr/tifelt/terrains_et_fermes/Quatre_hectares_enregistr√©s_avec_une_maison_et_un_puits_et_des_grenadiers_57044751.htm\n",
      "[37/38] Scraping https://www.avito.ma/fr/tifelt/terrains_et_fermes/Trois_hectares__bonne_affaire__carr√©e__√†_7km_de_Tiflet_56377328.htm\n",
      "[38/38] Scraping https://www.avito.ma/fr/tilila/appartements/Appartement_Tilila_56066371.htm\n",
      "üåê Trouv√© 38 annonces sur https://www.avito.ma/fr/maroc/locations_immobilieres-√†_louer\n",
      "[1/38] Scraping https://www.avito.ma/fr/2_mars/appartements/Appartement_√†_louer_avec__Une_grande_terrasse_56994067.htm\n",
      "[2/38] Scraping https://www.avito.ma/fr/abdelmoumen/bureaux/A_louer_bureau_100_m2_56197723.htm\n",
      "[3/38] Scraping https://www.avito.ma/fr/agdal/appartements/Appartement_√†_louer_√†_l_Agdal_56766162.htm\n",
      "[4/38] Scraping https://www.avito.ma/fr/agdal/bureaux/Plateau_Bureau_350_m¬≤_√†_louer_Agdal_Rabat_57076365.htm\n",
      "[5/38] Scraping https://www.avito.ma/fr/ain_sebaa/bureaux/Bureau_luxe_√©quip√©_√†_Louer_Ain_Sebaa_pret_de_2M_57131127.htm\n",
      "[6/38] Scraping https://www.avito.ma/fr/ancienne_m√©dina/local/Magasin_2_fa√ßades_Bab_Ghmat_54096705.htm\n",
      "[7/38] Scraping https://www.avito.ma/fr/autre_secteur/appartements/Appartement_√†_Louer_Meubl√©_Bouznika_Beach_57238284.htm\n",
      "[8/38] Scraping https://www.avito.ma/fr/autre_secteur/appartements/Hay_Charaf_proche_de_Marjane_57238371.htm\n",
      "[9/38] Scraping https://www.avito.ma/fr/autre_secteur/local/Magasin_√†_louer_bd_Ghandi_55403627.htm\n",
      "[10/38] Scraping https://www.avito.ma/fr/a√Øn_chock/local/Hangar_1100m2_√†_r√©gion_mkanssa_Ain_chock_56014175.htm\n",
      "[11/38] Scraping https://www.avito.ma/fr/belv√©d√®re/local/A_louer_un_grand_magasin_avec_contrat_de_bail_57130986.htm\n",
      "[12/38] Scraping https://www.avito.ma/fr/berrechid/appartements/Appartement_√†_louer_au_quartier_Tassir_2_pr√®s_de_la_mosqu√©e_Ibrahim_Khalil_57238334.htm\n",
      "[13/38] Scraping https://www.avito.ma/fr/berrechid/appartements/Appartement_√†_louer_pr√®s_d_Al_Abich_3_Mars_57238301.htm\n",
      "[14/38] Scraping https://www.avito.ma/fr/berrechid/appartements/Appartement_√†_louer_√†_Bir_Anzarane_57238338.htm\n",
      "[15/38] Scraping https://www.avito.ma/fr/berrechid/appartements/Appartement_√†_louer_√†_Hay_Layali_57238302.htm\n",
      "[16/38] Scraping https://www.avito.ma/fr/bourgogne/bureaux/Spa_√†_louer_Bourgogne_55757815.htm\n",
      "[17/38] Scraping https://www.avito.ma/fr/centre_ville/appartements/Bel_Appart_3ch_standing_140_m2_pre_de_parc_55593855.htm\n",
      "[18/38] Scraping https://www.avito.ma/fr/centre_ville/appartements/Studio_de_50m2_Mers_Sultan_location_57238332.htm\n",
      "[19/38] Scraping https://www.avito.ma/fr/dar_bouazza/appartements/Appartement_√†_louer_128_m¬≤_√†_Dar_Bouazza_57101562.htm\n",
      "[20/38] Scraping https://www.avito.ma/fr/gauthier/appartements/Studio_√†_louer_Gauthier_55663870.htm\n",
      "[21/38] Scraping https://www.avito.ma/fr/gu√©liz/bureaux/Plateaux_bureaux_√†_louer_56183999.htm\n",
      "[22/38] Scraping https://www.avito.ma/fr/hamria/local/√Ä_VENDRE_OU_A_LOUER_MAGAZIN_BOUTIQUE_BUREAU_57215875.htm\n",
      "[23/38] Scraping https://www.avito.ma/fr/hay_essalam/local/Un_magasin_√†_louer_57182018.htm\n",
      "[24/38] Scraping https://www.avito.ma/fr/hay_riad/appartements/Appartement_√†_louer_mahaj_riad_54684176.htm\n",
      "[25/38] Scraping https://www.avito.ma/fr/hay_riad/appartements/Appartement_√†_louer_√†_Hay_Riad___118_m¬≤_57188464.htm\n",
      "[26/38] Scraping https://www.avito.ma/fr/hay_salam/local/Local_commercial_√†_louer_57238356.htm\n",
      "[27/38] Scraping https://www.avito.ma/fr/maarif/local/Magasin_√†_louer_160m_56920617.htm\n",
      "[28/38] Scraping https://www.avito.ma/fr/maarif/local/Magasin_√†_louer_Maarif_56157336.htm\n",
      "[29/38] Scraping https://www.avito.ma/fr/massira_2/appartements/Appartement_√†_louer_√†_Temara_Proche_de_khaizoran_4_57084811.htm\n",
      "[30/38] Scraping https://www.avito.ma/fr/ma√¢rif_extension/local/Snack_√†_louer_Maarif_Extension_52743135.htm\n",
      "[31/38] Scraping https://www.avito.ma/fr/nassim/appartements/Appartements_√†_louer_60_m¬≤_√†_Casablanca_56743206.htm\n",
      "[32/38] Scraping https://www.avito.ma/fr/oasis/appartements/Studio_meubl√©_√†_Oasis___Route_El_Jadida_57131276.htm\n",
      "[33/38] Scraping https://www.avito.ma/fr/riviera/appartements/Studio_meubl√©_Riviera_52151097.htm\n",
      "[34/38] Scraping https://www.avito.ma/fr/sidi_maarouf/appartements/Appartement_√†_louer_55_m¬≤_√†_Casablanca_57238322.htm\n",
      "[35/38] Scraping https://www.avito.ma/fr/val_fleuri/appartements/Studio_meubl√©_Val_Fleuri_TRAMWAY_56620071.htm\n",
      "[36/38] Scraping https://www.avito.ma/fr/ville_verte/appartements/Villa_meubl√©e_√†_louer_Ville_Verte_55917199.htm\n",
      "[37/38] Scraping https://www.avito.ma/fr/wifak/appartements/Excellents_appartements_meubl√©s_√†_louer_56551641.htm\n",
      "[38/38] Scraping https://www.avito.ma/fr/zone_industrielle/autre_immobilier/Hangar_1000m2_√†_Mohammedia_56073590.htm\n",
      "Ventes r√©cup√©r√©es: 38\n",
      "Locations r√©cup√©r√©es: 38\n",
      "üìå Sch√©ma ventes\n",
      "root\n",
      " |-- attributes: string (nullable = true)\n",
      " |-- breadcrumbs: string (nullable = true)\n",
      " |-- breadcrumbs_list: string (nullable = true)\n",
      " |-- category_label: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- equipments: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- images: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- price_text: string (nullable = true)\n",
      " |-- published_date: string (nullable = true)\n",
      " |-- scraping_time: string (nullable = true)\n",
      " |-- seller_is_store: boolean (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- seller_url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- source_site: string (nullable = false)\n",
      " |-- offre: string (nullable = false)\n",
      " |-- ingest_ts: string (nullable = false)\n",
      "\n",
      "üìå Sch√©ma locations\n",
      "root\n",
      " |-- attributes: string (nullable = true)\n",
      " |-- breadcrumbs: string (nullable = true)\n",
      " |-- breadcrumbs_list: string (nullable = true)\n",
      " |-- category_label: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- equipments: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- images: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- price_text: string (nullable = true)\n",
      " |-- published_date: string (nullable = true)\n",
      " |-- scraping_time: string (nullable = true)\n",
      " |-- seller_is_store: boolean (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- seller_url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- source_site: string (nullable = false)\n",
      " |-- offre: string (nullable = false)\n",
      " |-- ingest_ts: string (nullable = false)\n",
      "\n",
      "‚úÖ Ventes Avito (PARQUET) -> abfss://realestate@strealestatehamza.dfs.core.windows.net/raw/avito/ventes/2025/11/25/001635\n",
      "‚úÖ Locations Avito (PARQUET) -> abfss://realestate@strealestatehamza.dfs.core.windows.net/raw/avito/locations/2025/11/25/001635\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELLULE 10 : Workflow Spark + ADLS (Avito, PARQUET)\n",
    "# ========================================\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "\n",
    "# üîí Variables stockage ADLS\n",
    "storage_account = \"strealestatehamza\"\n",
    "container = \"realestate\"\n",
    "\n",
    "# --- 1. URLs des listings Avito ---\n",
    "ventes_url = \"https://www.avito.ma/fr/maroc/ventes_immobilieres-√†_vendre\"\n",
    "locations_url = \"https://www.avito.ma/fr/maroc/locations_immobilieres-√†_louer\"\n",
    "\n",
    "# --- 2. Scraping -> listes de dicts ---\n",
    "raw_ventes_ads = scrape_listing_to_list(ventes_url)\n",
    "raw_locations_ads = scrape_listing_to_list(locations_url)\n",
    "\n",
    "print(f\"Ventes r√©cup√©r√©es: {len(raw_ventes_ads)}\")\n",
    "print(f\"Locations r√©cup√©r√©es: {len(raw_locations_ads)}\")\n",
    "\n",
    "\n",
    "def normalize_ads_for_spark(ads):\n",
    "    \"\"\"Convertit les champs dict/list en strings pour Spark (OK aussi pour Parquet).\"\"\"\n",
    "    normalized = []\n",
    "    for ad in ads:\n",
    "        d = ad.copy()\n",
    "        d[\"attributes\"] = json.dumps(d.get(\"attributes\", {}), ensure_ascii=False)\n",
    "        d[\"equipments\"] = \"; \".join(d.get(\"equipments\", []) or [])\n",
    "        d[\"breadcrumbs_list\"] = json.dumps(d.get(\"breadcrumbs_list\", []) or [], ensure_ascii=False)\n",
    "        d[\"images\"] = \", \".join(d.get(\"images\", []) or [])\n",
    "        normalized.append(d)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "ventes_ads = normalize_ads_for_spark(raw_ventes_ads)\n",
    "locations_ads = normalize_ads_for_spark(raw_locations_ads)\n",
    "\n",
    "# --- 3. Conversion en DataFrame Spark ---\n",
    "ventes_df = spark.createDataFrame(ventes_ads)\n",
    "locations_df = spark.createDataFrame(locations_ads)\n",
    "\n",
    "now = datetime.utcnow().isoformat()\n",
    "\n",
    "# Colonnes techniques\n",
    "ventes_df = (\n",
    "    ventes_df\n",
    "    .withColumn(\"source_site\", F.lit(\"avito\"))\n",
    "    .withColumn(\"offre\", F.lit(\"vente\"))\n",
    "    .withColumn(\"ingest_ts\", F.lit(now))\n",
    ")\n",
    "\n",
    "locations_df = (\n",
    "    locations_df\n",
    "    .withColumn(\"source_site\", F.lit(\"avito\"))\n",
    "    .withColumn(\"offre\", F.lit(\"location\"))\n",
    "    .withColumn(\"ingest_ts\", F.lit(now))\n",
    ")\n",
    "\n",
    "# --- Debug schemas ---\n",
    "print(\"üìå Sch√©ma ventes\")\n",
    "ventes_df.printSchema()\n",
    "print(\"üìå Sch√©ma locations\")\n",
    "locations_df.printSchema()\n",
    "\n",
    "# --- 4. Chemins RAW dans ADLS ---\n",
    "date_path = datetime.utcnow().strftime(\"%Y/%m/%d/%H%M%S\")\n",
    "base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/raw\"\n",
    "\n",
    "ventes_path = f\"{base_path}/avito/ventes/{date_path}\"\n",
    "locations_path = f\"{base_path}/avito/locations/{date_path}\"\n",
    "\n",
    "# --- 5. √âcriture en PARQUET (‚ùå plus de CSV) ---\n",
    "(\n",
    "    ventes_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(ventes_path)\n",
    ")\n",
    "\n",
    "(\n",
    "    locations_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(locations_path)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Ventes Avito (PARQUET) ->\", ventes_path)\n",
    "print(\"‚úÖ Locations Avito (PARQUET) ->\", locations_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Avito_scraper",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
