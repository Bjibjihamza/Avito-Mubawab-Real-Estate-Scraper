{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69c747-dc10-4763-aaf2-d65df13734cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avito_scraper_spark.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =========================================================\n",
    "# 1. ENV & SPARK CONFIG\n",
    "# =========================================================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# üîí ADLS / Storage config\n",
    "storage_account = \"strealestatehamza\"\n",
    "container = \"realestate\"\n",
    "\n",
    "adls_key = os.getenv(\"ADLS_ACCOUNT_KEY\")\n",
    "if not adls_key:\n",
    "    raise RuntimeError(\"ADLS_ACCOUNT_KEY missing from .env\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"RealEstate_Avito_Docker\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Set ADLS account keys (DFS + BLOB)\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    adls_key,\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n",
    "    adls_key,\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 2. CONSTANTS & BASIC UTILS\n",
    "# =========================================================\n",
    "\n",
    "BASE_URL = \"https://www.avito.ma\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    \"\"\"T√©l√©charge le HTML d'une page Avito.\"\"\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def extract_id_from_url(url: str) -> str:\n",
    "    \"\"\"Extrait l'ID num√©rique √† la fin de l'URL Avito.\"\"\"\n",
    "    m = re.search(r\"_([0-9]+)\\.htm$\", url.split(\"?\")[0])\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. BREADCRUMBS, CAT, LOCATION, DATE\n",
    "# =========================================================\n",
    "\n",
    "def get_breadcrumbs(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"Retourne breadcrumbs_list et breadcrumbs (string).\"\"\"\n",
    "    crumbs = []\n",
    "    ol = soup.find(\"ol\", class_=re.compile(r\"sc-16q833i-0\"))\n",
    "    if ol:\n",
    "        for li in ol.find_all(\"li\", class_=re.compile(r\"sc-16q833i-3\")):\n",
    "            span_or_a = li.find([\"a\", \"span\"])\n",
    "            if span_or_a:\n",
    "                text = span_or_a.get_text(strip=True)\n",
    "                if text:\n",
    "                    crumbs.append(text)\n",
    "\n",
    "    return {\n",
    "        \"breadcrumbs_list\": crumbs,\n",
    "        \"breadcrumbs\": \" > \".join(crumbs) if crumbs else \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def get_category_label(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    R√©cup√®re le libell√© de cat√©gorie du bloc 'Categorie'\n",
    "    (ex: 'Appartements, √† louer').\n",
    "    \"\"\"\n",
    "    cat_section = soup.find(\"div\", attrs={\"aria-label\": re.compile(r\"Category \")})\n",
    "    if not cat_section:\n",
    "        return \"\"\n",
    "\n",
    "    texts = cat_section.stripped_strings\n",
    "    for t in texts:\n",
    "        if \"Categorie\" in t:\n",
    "            continue\n",
    "        return t\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_location_and_date(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re location et published_date_raw.\n",
    "    Ajoute √©galement scraping_time.\n",
    "    \"\"\"\n",
    "    location = \"\"\n",
    "    published_date_raw = \"\"\n",
    "\n",
    "    # Date publication <time>\n",
    "    time_tag = soup.find(\"time\", attrs={\"datetime\": True})\n",
    "    if time_tag:\n",
    "        published_date_raw = time_tag[\"datetime\"]\n",
    "\n",
    "    # Location (ex: Racine, Casablanca)\n",
    "    location_span = None\n",
    "    for svg in soup.find_all(\"svg\", title=re.compile(r\"MapPinFill Icon\")):\n",
    "        parent = svg.parent\n",
    "        location_span = parent.find(\"span\", class_=re.compile(r\"sc-16573058-17\"))\n",
    "        if location_span:\n",
    "            break\n",
    "\n",
    "    if location_span:\n",
    "        location = location_span.get_text(strip=True)\n",
    "\n",
    "    # Temps de scraping\n",
    "    scraping_time = datetime.utcnow().isoformat()\n",
    "\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"published_date\": published_date_raw,\n",
    "        \"scraping_time\": scraping_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. TITLE, PRICE, DESCRIPTION, IMAGES\n",
    "# =========================================================\n",
    "\n",
    "def get_title_and_price(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"R√©cup√®re le titre (h1) et le prix.\"\"\"\n",
    "    title = \"\"\n",
    "    price_text = \"\"\n",
    "\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        title = h1.get_text(strip=True)\n",
    "\n",
    "    price_block = soup.find(\"div\", class_=re.compile(r\"sc-16573058-10\"))\n",
    "    if price_block:\n",
    "        p = price_block.find(\"p\")\n",
    "        if p:\n",
    "            price_text = p.get_text(strip=True)\n",
    "    else:\n",
    "        p = soup.find(string=re.compile(r\"DH\"))\n",
    "        if p:\n",
    "            price_text = p.strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price_text\": price_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_description(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"R√©cup√®re la description depuis le bloc Description.\"\"\"\n",
    "    desc_container = None\n",
    "    for div in soup.find_all(\"div\", class_=re.compile(r\"sc-b59a33d2-3\")):\n",
    "        h2 = div.find(\"h2\")\n",
    "        if h2 and \"Description\" in h2.get_text():\n",
    "            desc_container = div\n",
    "            break\n",
    "\n",
    "    if desc_container:\n",
    "        text_parts = []\n",
    "        for node in desc_container.find_all([\"p\", \"div\", \"span\"], recursive=True):\n",
    "            t = node.get_text(\" \", strip=True)\n",
    "            if t:\n",
    "                text_parts.append(t)\n",
    "        description = \" \".join(text_parts)\n",
    "        description = re.sub(r\"\\s+\", \" \", description).strip()\n",
    "        return description\n",
    "\n",
    "    # fallback\n",
    "    p = soup.find(\"p\")\n",
    "    if p:\n",
    "        return p.get_text(\" \", strip=True)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_images(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"R√©cup√®re toutes les URLs d'images de la galerie principale.\"\"\"\n",
    "    urls = []\n",
    "    for img in soup.select(\"div.picture img\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src and \"content.avito.ma/classifieds/images\" in src:\n",
    "            if src not in urls:\n",
    "                urls.append(src)\n",
    "    return urls\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. ATTRIBUTES, EQUIPMENTS, SELLER INFO\n",
    "# =========================================================\n",
    "\n",
    "def get_attributes_and_equipments(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re :\n",
    "      - attributes: dict { \"Chambres\": \"3\", \"Salle de bain\": \"2\", ... }\n",
    "      - equipments: liste [\"Ascenseur\", \"Balcon\", ...]\n",
    "    \"\"\"\n",
    "    attributes = {}\n",
    "    equipments = []\n",
    "\n",
    "    attr_blocks = soup.find_all(\"div\", class_=re.compile(r\"sc-cd1c365e-0\"))\n",
    "    for block in attr_blocks:\n",
    "        parent = block.find_parent(\"div\", class_=re.compile(r\"sc-b59a33d2-3\"))\n",
    "        is_equip_section = False\n",
    "        if parent:\n",
    "            h2 = parent.find(\"h2\")\n",
    "            if h2 and \"√âquipements\" in h2.get_text():\n",
    "                is_equip_section = True\n",
    "\n",
    "        for item in block.find_all(\"div\", class_=re.compile(r\"sc-cd1c365e-1\")):\n",
    "            img = item.find(\"img\", alt=True)\n",
    "            label_from_alt = img[\"alt\"].strip() if img else \"\"\n",
    "\n",
    "            value_span = item.find(\"span\", class_=re.compile(r\"fjZBup\"))\n",
    "            value = value_span.get_text(strip=True) if value_span else \"\"\n",
    "\n",
    "            if is_equip_section:\n",
    "                if label_from_alt:\n",
    "                    equipments.append(label_from_alt)\n",
    "                elif value:\n",
    "                    equipments.append(value)\n",
    "            else:\n",
    "                if label_from_alt and value:\n",
    "                    attributes[label_from_alt] = value\n",
    "\n",
    "    return {\n",
    "        \"attributes\": attributes,\n",
    "        \"equipments\": equipments,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_seller_info(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"R√©cup√®re seller_name, seller_url, seller_is_store.\"\"\"\n",
    "    seller_name = \"\"\n",
    "    seller_url = \"\"\n",
    "    seller_is_store = False\n",
    "\n",
    "    seller_block = soup.find(\"div\", attrs={\"data-test\": \"av_sellerInfo\"})\n",
    "    if not seller_block:\n",
    "        seller_block = soup.find(\"div\", class_=re.compile(r\"sc-1l0do2b-0\"))\n",
    "\n",
    "    if seller_block:\n",
    "        a = seller_block.find(\"a\", href=True)\n",
    "        if a:\n",
    "            seller_url = urljoin(BASE_URL, a[\"href\"])\n",
    "            name_tag = a.find(\"p\") or a.find(\"span\")\n",
    "            if name_tag:\n",
    "                seller_name = name_tag.get_text(strip=True)\n",
    "\n",
    "        text_block = seller_block.get_text(\" \", strip=True)\n",
    "        if \"Voir la boutique\" in text_block:\n",
    "            seller_is_store = True\n",
    "\n",
    "        if seller_block.find(\"title\", string=re.compile(r\"Store Icon\")):\n",
    "            seller_is_store = True\n",
    "\n",
    "    return {\n",
    "        \"seller_name\": seller_name,\n",
    "        \"seller_url\": seller_url,\n",
    "        \"seller_is_store\": seller_is_store,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. PARSER PRINCIPAL\n",
    "# =========================================================\n",
    "\n",
    "def parse_avito_ad(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"Scrape une annonce Avito et retourne un dict.\"\"\"\n",
    "    html = fetch_html(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    ad_id = extract_id_from_url(url)\n",
    "\n",
    "    crumbs = get_breadcrumbs(soup)\n",
    "    cat_label = get_category_label(soup)\n",
    "    loc_date = get_location_and_date(soup)\n",
    "    title_price = get_title_and_price(soup)\n",
    "    description = get_description(soup)\n",
    "    images = get_images(soup)\n",
    "    attrs_equip = get_attributes_and_equipments(soup)\n",
    "    seller = get_seller_info(soup)\n",
    "\n",
    "    ad_data: Dict[str, Any] = {\n",
    "        \"id\": ad_id,\n",
    "        \"url\": url,\n",
    "        \"title\": title_price[\"title\"],\n",
    "        \"price_text\": title_price[\"price_text\"],\n",
    "        \"location\": loc_date[\"location\"],\n",
    "        \"published_date\": loc_date[\"published_date\"],\n",
    "        \"scraping_time\": loc_date[\"scraping_time\"],\n",
    "        \"breadcrumbs_list\": crumbs[\"breadcrumbs_list\"],\n",
    "        \"breadcrumbs\": crumbs[\"breadcrumbs\"],\n",
    "        \"category_label\": cat_label,\n",
    "        \"description\": description,\n",
    "        \"images\": images,\n",
    "        \"attributes\": attrs_equip[\"attributes\"],\n",
    "        \"equipments\": attrs_equip[\"equipments\"],\n",
    "        \"seller_name\": seller[\"seller_name\"],\n",
    "        \"seller_url\": seller[\"seller_url\"],\n",
    "        \"seller_is_store\": seller[\"seller_is_store\"],\n",
    "    }\n",
    "\n",
    "    return ad_data\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. LISTING -> URLS (PAGINATION)\n",
    "# =========================================================\n",
    "\n",
    "def get_ad_urls_from_listing(listing_url: str, page: int = 1) -> List[str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re toutes les URLs d'annonces d'une page listing Avito.\n",
    "    `page` permet de naviguer via ?o=2, ?o=3, etc.\n",
    "    \"\"\"\n",
    "    if page == 1:\n",
    "        page_url = listing_url\n",
    "    else:\n",
    "        page_url = f\"{listing_url}?o={page}\"\n",
    "\n",
    "    html = fetch_html(page_url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    ad_urls = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "\n",
    "        if href.startswith(\"/\"):\n",
    "            href = urljoin(BASE_URL, href)\n",
    "\n",
    "        if \"avito.ma\" not in href:\n",
    "            continue\n",
    "\n",
    "        if re.search(r\"_[0-9]+\\.htm$\", href):\n",
    "            clean = href.split(\"?\")[0]\n",
    "            ad_urls.add(clean)\n",
    "\n",
    "    ad_urls = sorted(ad_urls)\n",
    "    print(f\"üåê Trouv√© {len(ad_urls)} annonces sur {page_url}\")\n",
    "    return ad_urls\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. SCRAPER LISTING -> LISTE (MULTI PAGES)\n",
    "# =========================================================\n",
    "\n",
    "def scrape_listing_to_list(listing_url: str, max_pages: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scrape jusqu'√† `max_pages` pages d'un listing Avito\n",
    "    et retourne une liste de dicts.\n",
    "    \"\"\"\n",
    "    all_ads: List[Dict[str, Any]] = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"\\n========== PAGE {page}/{max_pages} ==========\")\n",
    "        try:\n",
    "            ad_urls = get_ad_urls_from_listing(listing_url, page=page)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors du chargement de la page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        if not ad_urls:\n",
    "            print(\"Aucune annonce trouv√©e sur cette page, arr√™t du scraping.\")\n",
    "            break\n",
    "\n",
    "        new_urls = [u for u in ad_urls if u not in seen_urls]\n",
    "        if not new_urls:\n",
    "            print(\"Toutes les annonces de cette page sont d√©j√† vues, arr√™t du scraping.\")\n",
    "            break\n",
    "\n",
    "        seen_urls.update(new_urls)\n",
    "        total = len(new_urls)\n",
    "\n",
    "        for i, ad_url in enumerate(new_urls, 1):\n",
    "            print(f\"[Page {page}] [{i}/{total}] Scraping {ad_url}\")\n",
    "            try:\n",
    "                ad_data = parse_avito_ad(ad_url)\n",
    "                all_ads.append(ad_data)\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Erreur sur {ad_url}: {e}\")\n",
    "\n",
    "            time.sleep(random.uniform(1.0, 2.5))  # Respect du site\n",
    "\n",
    "    print(f\"\\n‚úÖ Total annonces collect√©es depuis {listing_url}: {len(all_ads)}\")\n",
    "    return all_ads\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. NORMALISATION POUR SPARK\n",
    "# =========================================================\n",
    "\n",
    "def normalize_ads_for_spark(ads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Convertit les champs dict/list en strings pour Spark (OK aussi pour Parquet).\"\"\"\n",
    "    normalized = []\n",
    "    for ad in ads:\n",
    "        d = ad.copy()\n",
    "        d[\"attributes\"] = json.dumps(d.get(\"attributes\", {}), ensure_ascii=False)\n",
    "        d[\"equipments\"] = \"; \".join(d.get(\"equipments\", []) or [])\n",
    "        d[\"breadcrumbs_list\"] = json.dumps(\n",
    "            d.get(\"breadcrumbs_list\", []) or [],\n",
    "            ensure_ascii=False\n",
    "        )\n",
    "        d[\"images\"] = \", \".join(d.get(\"images\", []) or [])\n",
    "        normalized.append(d)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. MAIN WORKFLOW SPARK + ADLS (PARQUET)\n",
    "# =========================================================\n",
    "\n",
    "def main():\n",
    "    # --- URLs des listings Avito ---\n",
    "    ventes_url = \"https://www.avito.ma/fr/maroc/ventes_immobilieres-√†_vendre\"\n",
    "    locations_url = \"https://www.avito.ma/fr/maroc/locations_immobilieres-√†_louer\"\n",
    "\n",
    "    # --- Scraping (50 pages chacune) ---\n",
    "    raw_ventes_ads = scrape_listing_to_list(ventes_url, max_pages=50)\n",
    "    raw_locations_ads = scrape_listing_to_list(locations_url, max_pages=50)\n",
    "\n",
    "    print(f\"Ventes r√©cup√©r√©es: {len(raw_ventes_ads)}\")\n",
    "    print(f\"Locations r√©cup√©r√©es: {len(raw_locations_ads)}\")\n",
    "\n",
    "    # --- Normalisation ---\n",
    "    ventes_ads = normalize_ads_for_spark(raw_ventes_ads)\n",
    "    locations_ads = normalize_ads_for_spark(raw_locations_ads)\n",
    "\n",
    "    # --- DataFrames Spark ---\n",
    "    ventes_df = spark.createDataFrame(ventes_ads) if ventes_ads else spark.createDataFrame([], schema=None)\n",
    "    locations_df = spark.createDataFrame(locations_ads) if locations_ads else spark.createDataFrame([], schema=None)\n",
    "\n",
    "    now_iso = datetime.utcnow().isoformat()\n",
    "\n",
    "    if ventes_ads:\n",
    "        ventes_df = (\n",
    "            ventes_df\n",
    "            .withColumn(\"source_site\", F.lit(\"avito\"))\n",
    "            .withColumn(\"offre\", F.lit(\"vente\"))\n",
    "            .withColumn(\"ingest_ts\", F.lit(now_iso))\n",
    "        )\n",
    "\n",
    "    if locations_ads:\n",
    "        locations_df = (\n",
    "            locations_df\n",
    "            .withColumn(\"source_site\", F.lit(\"avito\"))\n",
    "            .withColumn(\"offre\", F.lit(\"location\"))\n",
    "            .withColumn(\"ingest_ts\", F.lit(now_iso))\n",
    "        )\n",
    "\n",
    "    if ventes_ads:\n",
    "        print(\"üìå Sch√©ma ventes\")\n",
    "        ventes_df.printSchema()\n",
    "    if locations_ads:\n",
    "        print(\"üìå Sch√©ma locations\")\n",
    "        locations_df.printSchema()\n",
    "\n",
    "    # --- ADLS paths ---\n",
    "    date_path = datetime.utcnow().strftime(\"%Y/%m/%d/%H%M%S\")\n",
    "    base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/raw\"\n",
    "\n",
    "    ventes_path = f\"{base_path}/avito/ventes/{date_path}\"\n",
    "    locations_path = f\"{base_path}/avito/locations/{date_path}\"\n",
    "\n",
    "    # --- √âcriture Parquet ---\n",
    "    if ventes_ads:\n",
    "        (\n",
    "            ventes_df\n",
    "            .coalesce(1)\n",
    "            .write\n",
    "            .mode(\"overwrite\")\n",
    "            .parquet(ventes_path)\n",
    "        )\n",
    "        print(\"‚úÖ Ventes Avito (PARQUET) ->\", ventes_path)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucune vente √† √©crire.\")\n",
    "\n",
    "    if locations_ads:\n",
    "        (\n",
    "            locations_df\n",
    "            .coalesce(1)\n",
    "            .write\n",
    "            .mode(\"overwrite\")\n",
    "            .parquet(locations_path)\n",
    "        )\n",
    "        print(\"‚úÖ Locations Avito (PARQUET) ->\", locations_path)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucune location √† √©crire.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee2390-b610-41de-9894-7e8c06afa0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
