{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa816eb6-3dfa-488e-a3ee-1ac395177555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealEstate_Mubawab_Docker\").getOrCreate()\n",
    "\n",
    "storage_account = \"strealestatehamza\"\n",
    "container = \"realestate\"\n",
    "\n",
    "adls_key = os.getenv(\"ADLS_ACCOUNT_KEY\")\n",
    "if not adls_key:\n",
    "    raise RuntimeError(\"ADLS_ACCOUNT_KEY missing from .env\")\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    adls_key,\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n",
    "    adls_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b2c08b-52e1-4a7a-b781-273b49fa4dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 1 : \n",
    "# ========================================\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://www.mubawab.ma\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0 Safari/537.36\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e625d3e7-637d-4b8c-a0a4-cbd4210127e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 2 : Fonctions basiques\n",
    "# ========================================\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    \"\"\"T√©l√©charge le HTML d'une page Mubawab.\"\"\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def extract_id_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrait l'ID num√©rique Mubawab depuis une URL du type:\n",
    "      https://www.mubawab.ma/fr/a/8256920/...\n",
    "    \"\"\"\n",
    "    m = re.search(r\"/a/(\\d+)\", url.split(\"?\")[0])\n",
    "    return m.group(1) if m else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331d3f51-da75-42b8-ade6-99b17307ebfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 3 CORRIG√âE : breadcrumbs, cat√©gorie, localisation & date\n",
    "# ========================================\n",
    "\n",
    "def get_breadcrumbs(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retourne breadcrumbs_list et breadcrumbs (string).\n",
    "    Extrait depuis le div.adBread selon le HTML fourni.\n",
    "    \"\"\"\n",
    "    crumbs: List[str] = []\n",
    "\n",
    "    # 1) Chercher le div.adBread sp√©cifique √† Mubawab\n",
    "    ad_bread = soup.find(\"div\", class_=re.compile(r\"adBread\"))\n",
    "    if ad_bread:\n",
    "        # Extraire tous les liens <a> dans ce div\n",
    "        for a in ad_bread.find_all(\"a\", class_=\"darkblue\", href=True):\n",
    "            text = a.get_text(strip=True)\n",
    "            if text:\n",
    "                crumbs.append(text)\n",
    "\n",
    "    # 2) Fallback: essai classique ul.breadcrumb li a\n",
    "    if not crumbs:\n",
    "        ul = soup.find(\"ul\", class_=re.compile(r\"breadcrumb\"))\n",
    "        if ul:\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                a = li.find(\"a\")\n",
    "                text = (a or li).get_text(strip=True)\n",
    "                if text:\n",
    "                    crumbs.append(text)\n",
    "\n",
    "    # 3) Fallback: nav[aria-label=\"Fil d'ariane\"]\n",
    "    if not crumbs:\n",
    "        nav = soup.find(\"nav\", attrs={\"aria-label\": re.compile(\"Fil\", re.I)})\n",
    "        if nav:\n",
    "            for li in nav.find_all(\"li\"):\n",
    "                text = li.get_text(strip=True)\n",
    "                if text:\n",
    "                    crumbs.append(text)\n",
    "\n",
    "    return {\n",
    "        \"breadcrumbs_list\": crumbs,\n",
    "        \"breadcrumbs\": \" > \".join(crumbs) if crumbs else \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def get_category_label(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    R√©cup√®re un libell√© de cat√©gorie :\n",
    "        ex: 'Villa, √† vendre' ou 'Appartement, √† louer'\n",
    "    \"\"\"\n",
    "    cat_label = \"\"\n",
    "    type_bien = \"\"\n",
    "\n",
    "    # R√©cup√©ration du bloc Caract√©ristiques g√©n√©rales\n",
    "    for feature in soup.select(\"div.caractBlockProp div.adMainFeature\"):\n",
    "        label_el = feature.select_one(\"p.adMainFeatureContentLabel\")\n",
    "        value_el = feature.select_one(\"p.adMainFeatureContentValue\")\n",
    "        if not label_el or not value_el:\n",
    "            continue\n",
    "        label = label_el.get_text(strip=True)\n",
    "        value = value_el.get_text(strip=True)\n",
    "\n",
    "        if \"Type de bien\" in label:\n",
    "            type_bien = value\n",
    "            break\n",
    "\n",
    "    # Essai de deviner louer/vendre √† partir du texte de page\n",
    "    full_text = soup.get_text(\" \", strip=True).lower()\n",
    "    suffix = \"\"\n",
    "    if \"√† louer\" in full_text or \"a louer\" in full_text:\n",
    "        suffix = \", √† louer\"\n",
    "    elif \"√† vendre\" in full_text or \"a vendre\" in full_text:\n",
    "        suffix = \", √† vendre\"\n",
    "\n",
    "    if type_bien:\n",
    "        cat_label = type_bien + suffix\n",
    "    return cat_label\n",
    "\n",
    "\n",
    "def get_location_and_date(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re:\n",
    "      - location: ex 'Hay Targa √† Marrakech'\n",
    "      - published_date: date/heure de scraping (maintenant)\n",
    "      - scraping_time: timestamp exact du scraping\n",
    "    \"\"\"\n",
    "    location = \"\"\n",
    "    \n",
    "    # Location: ex <h3 class=\"greyTit\">Gu√©liz √† Marrakech</h3>\n",
    "    grey = soup.find(\"h3\", class_=re.compile(r\"greyTit\"))\n",
    "    if grey:\n",
    "        location = grey.get_text(\" \", strip=True)\n",
    "\n",
    "    # Date de publication = temps de scraping\n",
    "    scraping_time = datetime.utcnow()\n",
    "    published_date = scraping_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"published_date\": published_date,\n",
    "        \"scraping_time\": scraping_time.isoformat(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afe8900-5410-41ca-a66f-df617c9154fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 4 : titre, prix, description, images\n",
    "# ========================================\n",
    "\n",
    "def get_title_and_price(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re le titre (h1.searchTitle) et le prix (h3.orangeTit).\n",
    "    \"\"\"\n",
    "    title = \"\"\n",
    "    price_text = \"\"\n",
    "\n",
    "    # Titre\n",
    "    h1 = soup.find(\"h1\", class_=re.compile(r\"searchTitle\"))\n",
    "    if h1:\n",
    "        title = h1.get_text(strip=True)\n",
    "\n",
    "    # Prix : h3.orangeTit\n",
    "    price_block = soup.find(\"h3\", class_=re.compile(r\"orangeTit\"))\n",
    "    if price_block:\n",
    "        price_text = price_block.get_text(strip=True)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price_text\": price_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_description(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    R√©cup√®re la description dans le bloc:\n",
    "      <div class=\"blockProp\"><h1 class=\"searchTitle\">..</h1><p>...</p></div>\n",
    "    \"\"\"\n",
    "    desc = \"\"\n",
    "\n",
    "    # Bloc contenant la description (celui qui a un <h1 class=\"searchTitle\">)\n",
    "    for block in soup.find_all(\"div\", class_=re.compile(r\"blockProp\")):\n",
    "        h1 = block.find(\"h1\", class_=re.compile(r\"searchTitle\"))\n",
    "        if h1:\n",
    "            p = block.find(\"p\")\n",
    "            if p:\n",
    "                desc = p.get_text(\" \", strip=True)\n",
    "            break\n",
    "\n",
    "    if not desc:\n",
    "        # fallback: premier paragraphe long\n",
    "        p = soup.find(\"p\")\n",
    "        if p:\n",
    "            desc = p.get_text(\" \", strip=True)\n",
    "\n",
    "    desc = re.sub(r\"\\s+\", \" \", desc).strip()\n",
    "    return desc\n",
    "\n",
    "\n",
    "def get_images(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re toutes les URLs d'images de la galerie principale Mubawab.\n",
    "    On cible les URLs 'mubawab-media.com/ad/...'.\n",
    "    \"\"\"\n",
    "    urls: List[str] = []\n",
    "\n",
    "    # Overlay principale\n",
    "    for img in soup.select(\"div.overlayPhoto img[src]\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src and \"mubawab-media.com/ad/\" in src and src not in urls:\n",
    "            urls.append(src)\n",
    "\n",
    "    # Galerie\n",
    "    for img in soup.select(\"#picturesGallery img[src]\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src and \"mubawab-media.com/ad/\" in src and src not in urls:\n",
    "            urls.append(src)\n",
    "\n",
    "    # Slider\n",
    "    for img in soup.select(\"#picturesSlider img[src]\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src and \"mubawab-media.com/ad/\" in src and src not in urls:\n",
    "            urls.append(src)\n",
    "\n",
    "    # Fallback g√©n√©rique\n",
    "    for img in soup.find_all(\"img\", src=True):\n",
    "        src = img[\"src\"]\n",
    "        if \"mubawab-media.com/ad/\" in src and src not in urls:\n",
    "            urls.append(src)\n",
    "\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b91c7e-e907-40ea-99c7-8f32bd7b2d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 5 : attributs, √©quipements, infos vendeur\n",
    "# ========================================\n",
    "\n",
    "def get_attributes_and_equipments(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re :\n",
    "      - attributes: dict { \"Type de bien\": \"Villa\", \"Etat\": \"Bon √©tat\", ... }\n",
    "      - equipments: liste [\"Terrasse\", \"Piscine\", \"Climatisation\", ...]\n",
    "    \"\"\"\n",
    "    attributes: Dict[str, str] = {}\n",
    "    equipments: List[str] = []\n",
    "\n",
    "    # 1) Caract√©ristiques g√©n√©rales\n",
    "    for feature in soup.select(\"div.caractBlockProp div.adMainFeature\"):\n",
    "        label_el = feature.select_one(\"p.adMainFeatureContentLabel\")\n",
    "        value_el = feature.select_one(\"p.adMainFeatureContentValue\")\n",
    "        if not label_el or not value_el:\n",
    "            continue\n",
    "        label = label_el.get_text(\" \", strip=True)\n",
    "        value = value_el.get_text(\" \", strip=True)\n",
    "        if label and value:\n",
    "            attributes[label] = value\n",
    "\n",
    "    # 2) D√©tails surface / SDB / etc. dans le header (ic√¥nes triangles, etc.)\n",
    "    for span in soup.select(\"div.adDetails div.adDetailFeature span\"):\n",
    "        txt = span.get_text(\" \", strip=True)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # mapping simple\n",
    "        if \"m¬≤\" in txt or \"m2\" in txt:\n",
    "            key = \"Surface\"\n",
    "        elif \"Salle de bain\" in txt or \"Salles de bains\" in txt:\n",
    "            key = \"Salle de bain\"\n",
    "        elif \"Pi√®ce\" in txt:\n",
    "            key = \"Pi√®ces\"\n",
    "        else:\n",
    "            key = txt  # on garde tel quel\n",
    "\n",
    "        attributes[key] = txt\n",
    "\n",
    "    # 3) Coordonn√©es + locationId/locationType (on les range aussi dans attributes)\n",
    "    lat_el = soup.select_one(\"input#latField\")\n",
    "    lng_el = soup.select_one(\"input#lngField\")\n",
    "    loc_id_el = soup.select_one(\"input#locationId\")\n",
    "    loc_type_el = soup.select_one(\"input#locationType\")\n",
    "\n",
    "    if lat_el and lat_el.get(\"value\"):\n",
    "        attributes[\"Latitude\"] = lat_el[\"value\"].strip()\n",
    "    if lng_el and lng_el.get(\"value\"):\n",
    "        attributes[\"Longitude\"] = lng_el[\"value\"].strip()\n",
    "    if loc_id_el and loc_id_el.get(\"value\"):\n",
    "        attributes[\"Location ID\"] = loc_id_el[\"value\"].strip()\n",
    "    if loc_type_el and loc_type_el.get(\"value\"):\n",
    "        attributes[\"Location Type\"] = loc_type_el[\"value\"].strip()\n",
    "\n",
    "    # 4) √âquipements (petites ic√¥nes sous Caract√©ristiques g√©n√©rales)\n",
    "    for feat in soup.select(\"div.caractBlockProp div.adFeatures div.adFeature span\"):\n",
    "        txt = feat.get_text(\" \", strip=True)\n",
    "        if txt:\n",
    "            equipments.append(txt)\n",
    "\n",
    "    return {\n",
    "        \"attributes\": attributes,\n",
    "        \"equipments\": equipments,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_seller_info(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re seller_name, seller_url, seller_is_store (Agence vs Particulier).\n",
    "    \"\"\"\n",
    "    seller_name = \"\"\n",
    "    seller_url = \"\"\n",
    "    seller_is_store = False  # True si agence / pro\n",
    "\n",
    "    business_info = soup.select_one(\"div.businessInfo\")\n",
    "    if business_info:\n",
    "        name_el = business_info.select_one(\"span.businessName\")\n",
    "        if name_el:\n",
    "            full_txt = name_el.get_text(\" \", strip=True)\n",
    "            txt_lower = full_txt.lower()\n",
    "\n",
    "            if \"particulier\" in txt_lower:\n",
    "                seller_is_store = False\n",
    "                seller_name = (\n",
    "                    full_txt.replace(\"Particulier\", \"\")\n",
    "                    .replace(\"particulier\", \"\")\n",
    "                    .strip(\" -|,\")\n",
    "                )\n",
    "            elif \"agence\" in txt_lower:\n",
    "                seller_is_store = True\n",
    "                seller_name = (\n",
    "                    full_txt.replace(\"Agence\", \"\")\n",
    "                    .replace(\"agence\", \"\")\n",
    "                    .strip(\" -|,\")\n",
    "                )\n",
    "            else:\n",
    "                seller_name = full_txt\n",
    "\n",
    "        # lien vers la page agence / pro s'il existe\n",
    "        a = business_info.find(\"a\", href=True)\n",
    "        if a:\n",
    "            seller_url = urljoin(BASE_URL, a[\"href\"])\n",
    "\n",
    "    return {\n",
    "        \"seller_name\": seller_name,\n",
    "        \"seller_url\": seller_url,\n",
    "        \"seller_is_store\": seller_is_store,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1ce5e5-7f3a-4f41-a4c2-9e54d517412b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 6\n",
    "# ========================================\n",
    "def parse_mubawab_ad(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape une annonce Mubawab (page /fr/a/...) et retourne un dict.\n",
    "    \"\"\"\n",
    "    html = fetch_html(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    ad_id = extract_id_from_url(url)\n",
    "\n",
    "    crumbs = get_breadcrumbs(soup)\n",
    "    cat_label = get_category_label(soup)\n",
    "    loc_date = get_location_and_date(soup)\n",
    "    title_price = get_title_and_price(soup)\n",
    "    description = get_description(soup)\n",
    "    images = get_images(soup)\n",
    "    attrs_equip = get_attributes_and_equipments(soup)\n",
    "    seller = get_seller_info(soup)\n",
    "\n",
    "    ad_data: Dict[str, Any] = {\n",
    "        \"id\": ad_id,\n",
    "        \"url\": url,\n",
    "        \"title\": title_price[\"title\"],\n",
    "        \"price_text\": title_price[\"price_text\"],\n",
    "        \"location\": loc_date[\"location\"],\n",
    "        \"published_date\": loc_date[\"published_date\"],\n",
    "        \"scraping_time\": loc_date[\"scraping_time\"],\n",
    "        \"breadcrumbs_list\": crumbs[\"breadcrumbs_list\"],\n",
    "        \"breadcrumbs\": crumbs[\"breadcrumbs\"],\n",
    "        \"category_label\": cat_label,\n",
    "        \"description\": description,\n",
    "        \"images\": images,\n",
    "        \"attributes\": attrs_equip[\"attributes\"],\n",
    "        \"equipments\": attrs_equip[\"equipments\"],\n",
    "        \"seller_name\": seller[\"seller_name\"],\n",
    "        \"seller_url\": seller[\"seller_url\"],\n",
    "        \"seller_is_store\": seller[\"seller_is_store\"],\n",
    "    }\n",
    "\n",
    "    return ad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93795aba-42be-4165-8e30-85188d1fee6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 7 : extraire URLs depuis listing\n",
    "# ========================================\n",
    "\n",
    "def get_ad_urls_from_listing(listing_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re toutes les URLs d'annonces d'une page listing Mubawab.\n",
    "    \"\"\"\n",
    "    html = fetch_html(listing_url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    ad_urls = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "\n",
    "        # Lien relatif ‚Üí absolu\n",
    "        if href.startswith(\"/\"):\n",
    "            href = urljoin(BASE_URL, href)\n",
    "\n",
    "        # Garder seulement les liens Mubawab\n",
    "        if \"mubawab.ma\" not in href:\n",
    "            continue\n",
    "\n",
    "        # Garder seulement les liens d'annonces /fr/a/\n",
    "        if \"/fr/a/\" in href:\n",
    "            clean = href.split(\"?\")[0]\n",
    "            ad_urls.add(clean)\n",
    "\n",
    "    ad_urls = sorted(ad_urls)\n",
    "    print(f\"üåê Trouv√© {len(ad_urls)} annonces sur {listing_url}\")\n",
    "    return ad_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba48870f-d199-4a8c-8260-8463a30b99fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# CELLULE 8 :\n",
    "# ========================================\n",
    "def save_ads_to_csv(ad_dicts: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"Sauvegarde une liste d'annonces (dict) dans un fichier CSV.\"\"\"\n",
    "    if not ad_dicts:\n",
    "        print(f\"Aucune annonce √† sauvegarder pour {filename}\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\n",
    "        \"id\",\n",
    "        \"url\",\n",
    "        \"title\",\n",
    "        \"price_text\",\n",
    "        \"location\",\n",
    "        \"published_date\",\n",
    "        \"scraping_time\",\n",
    "        \"breadcrumbs\",\n",
    "        \"breadcrumbs_list\",\n",
    "        \"category_label\",\n",
    "        \"description\",\n",
    "        \"attributes\",\n",
    "        \"equipments\",\n",
    "        \"seller_name\",\n",
    "        \"seller_url\",\n",
    "        \"seller_is_store\",\n",
    "    ]\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for ad in ad_dicts:\n",
    "            row: Dict[str, Any] = {}\n",
    "\n",
    "            for key in fieldnames:\n",
    "                if key in (\"attributes\", \"equipments\", \"breadcrumbs_list\"):\n",
    "                    continue\n",
    "                row[key] = ad.get(key, \"\")\n",
    "\n",
    "            row[\"attributes\"] = json.dumps(\n",
    "                ad.get(\"attributes\", {}), ensure_ascii=False\n",
    "            )\n",
    "            row[\"equipments\"] = \"; \".join(ad.get(\"equipments\", []))\n",
    "            row[\"breadcrumbs_list\"] = json.dumps(\n",
    "                ad.get(\"breadcrumbs_list\", []), ensure_ascii=False\n",
    "            )\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"‚úÖ Sauvegard√© {len(ad_dicts)} annonces dans {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540f7dbd-f25e-4297-ab86-d2956edbfffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 9 : scraper listing -> liste\n",
    "# ========================================\n",
    "\n",
    "def scrape_listing_to_list(listing_url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scrape la 1√®re page d'un listing Mubawab et retourne une liste de dicts.\n",
    "    \"\"\"\n",
    "    ad_urls = get_ad_urls_from_listing(listing_url)\n",
    "\n",
    "    all_ads: List[Dict[str, Any]] = []\n",
    "    total = len(ad_urls)\n",
    "\n",
    "    for i, ad_url in enumerate(ad_urls, 1):\n",
    "        print(f\"[{i}/{total}] Scraping {ad_url}\")\n",
    "        try:\n",
    "            ad_data = parse_mubawab_ad(ad_url)\n",
    "            all_ads.append(ad_data)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Erreur sur {ad_url}: {e}\")\n",
    "\n",
    "        time.sleep(random.uniform(1.0, 2.5))\n",
    "\n",
    "    return all_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac28ebe-2f97-4a8d-a8ec-0110f1941815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELLULE 10 : Write RAW Mubawab in PARQUET (RECOMMENDED)\n",
    "# ========================================\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---- ADLS config ----\n",
    "storage_account = \"strealestatehamza\"\n",
    "container = \"realestate\"\n",
    "\n",
    "# ---- Scraping listing URLs ----\n",
    "ventes_url = (\n",
    "    \"https://www.mubawab.ma/fr/cc/\"\n",
    "    \"immobilier-a-vendre-all:o:n:sc:\"\n",
    "    \"apartment-sale,commercial-sale,farm-sale,house-sale,land-sale,\"\n",
    "    \"office-sale,other-sale,riad-sale,villa-sale\"\n",
    ")\n",
    "\n",
    "locations_url = (\n",
    "    \"https://www.mubawab.ma/fr/cc/\"\n",
    "    \"immobilier-a-louer-all:o:n:sc:\"\n",
    "    \"apartment-rent,commercial-rent,farm-rent,house-rent,land-rent,\"\n",
    "    \"office-rent,other-rent,riad-rent,room-rent,villa-rent\"\n",
    ")\n",
    "\n",
    "# ---- Scrape ----\n",
    "raw_ventes_ads = scrape_listing_to_list(ventes_url)\n",
    "raw_locations_ads = scrape_listing_to_list(locations_url)\n",
    "\n",
    "print(f\"Ventes r√©cup√©r√©es: {len(raw_ventes_ads)}\")\n",
    "print(f\"Locations r√©cup√©r√©es: {len(raw_locations_ads)}\")\n",
    "\n",
    "# ---- Create Spark DataFrames directly from Python dicts ----\n",
    "ventes_df = spark.createDataFrame(raw_ventes_ads)\n",
    "locations_df = spark.createDataFrame(raw_locations_ads)\n",
    "\n",
    "now = datetime.utcnow().isoformat()\n",
    "\n",
    "ventes_df = (\n",
    "    ventes_df\n",
    "    .withColumn(\"source_site\", F.lit(\"mubawab\"))\n",
    "    .withColumn(\"offre\", F.lit(\"vente\"))\n",
    "    .withColumn(\"ingest_ts\", F.lit(now))\n",
    ")\n",
    "\n",
    "locations_df = (\n",
    "    locations_df\n",
    "    .withColumn(\"source_site\", F.lit(\"mubawab\"))\n",
    "    .withColumn(\"offre\", F.lit(\"location\"))\n",
    "    .withColumn(\"ingest_ts\", F.lit(now))\n",
    ")\n",
    "\n",
    "ventes_df.printSchema()\n",
    "locations_df.printSchema()\n",
    "\n",
    "# ---- Build ADLS path ----\n",
    "date_path = datetime.utcnow().strftime(\"%Y/%m/%d/%H%M%S\")\n",
    "base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/raw\"\n",
    "\n",
    "ventes_path = f\"{base_path}/mubawab/ventes/{date_path}\"\n",
    "locations_path = f\"{base_path}/mubawab/locations/{date_path}\"\n",
    "\n",
    "# ---- WRITE AS PARQUET (NOT CSV) ----\n",
    "(\n",
    "    ventes_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(ventes_path)\n",
    ")\n",
    "\n",
    "(\n",
    "    locations_df\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(locations_path)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAW Mubawab Ventes (PARQUET) ->\", ventes_path)\n",
    "print(\"‚úÖ RAW Mubawab Locations (PARQUET) ->\", locations_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mubwab_scraper",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
